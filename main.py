# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12KWCpk7aWl-Yf477timomW88Xd9ppKzS
"""

from flask import Flask, render_template, jsonify
import json
from datetime import datetime
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import time as time_module
from scipy.stats import kstest, wasserstein_distance
import seaborn as sns
from statsmodels.tsa.stattools import adfuller
import warnings
from sqlalchemy import create_engine, text
import os
from threading import Thread
import logging
from sqlalchemy.orm import sessionmaker

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

warnings.filterwarnings('ignore')
app = Flask(__name__)

DATABASE_URI = os.environ.get('DATABASE_URL')
if not DATABASE_URI:
    raise ValueError("DATABASE_URL environment variable not set")

# Поправка, если используется префикс postgres:// (Heroku и т.п.)
if DATABASE_URI.startswith('postgres://'):
    DATABASE_URI = DATABASE_URI.replace('postgres://', 'postgresql://')

engine = create_engine(DATABASE_URI)

def wait_for_db():
    max_retries = 30
    retry_interval = 1

    for i in range(max_retries):
        try:
            with engine.connect() as connection:
                connection.execute(text("SELECT 1"))
                logger.info("Successfully connected to the database")

                # Проверяем существование таблиц
                create_initial_tables(engine)
                logger.info("Tables created successfully")
                return True
        except Exception as e:
            logger.error(f"Attempt {i + 1}/{max_retries} to connect to the database failed: {e}")
            time_module.sleep(retry_interval)

    raise Exception("Could not connect to the database after maximum retries")

def create_initial_tables(engine):
    logger.info("Creating initial tables...")
    with engine.begin() as connection:
        connection.execute(text("""
            DROP TABLE IF EXISTS transactions;
            CREATE TABLE transactions (
                "time" TIMESTAMP,
                "v1" REAL, "v2" REAL, "v3" REAL, "v4" REAL, "v5" REAL,
                "v6" REAL, "v7" REAL, "v8" REAL, "v9" REAL, "v10" REAL,
                "v11" REAL, "v12" REAL, "v13" REAL, "v14" REAL, "v15" REAL,
                "v16" REAL, "v17" REAL, "v18" REAL, "v19" REAL, "v20" REAL,
                "v21" REAL, "v22" REAL, "v23" REAL, "v24" REAL, "v25" REAL,
                "v26" REAL, "v27" REAL, "v28" REAL,
                "amount" REAL,
                "class" INTEGER,
                "predicted_class" INTEGER
            );
        """))
        connection.execute(text("""
            DROP TABLE IF EXISTS model_metrics;
            CREATE TABLE model_metrics (
                "time" TIMESTAMP,
                "precision" REAL,
                "recall" REAL,
                "f1" REAL,
                "accuracy" REAL
            );
        """))
        connection.execute(text("""
            DROP TABLE IF EXISTS data_statistics;
            CREATE TABLE data_statistics (
                "time" TIMESTAMP,
                "feature" TEXT,
                "mean" REAL,
                "std" REAL
            );
        """))
    logger.info("Tables created successfully")

def calculate_metrics(y_true, y_pred):
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    accuracy = np.mean(y_true == y_pred)
    return precision, recall, f1, accuracy

def log_metrics_to_db(engine, precision, recall, f1, accuracy):
    logger.info(f"Logging metrics: precision={precision}, recall={recall}, f1={f1}, accuracy={accuracy}")
    with engine.begin() as connection:
        connection.execute(text("""
            INSERT INTO model_metrics (time, precision, recall, f1, accuracy)
            VALUES (:time, :precision, :recall, :f1, :accuracy)
        """),
            {"time": pd.Timestamp.now(),
             "precision": float(precision),
             "recall": float(recall),
             "f1": float(f1),
             "accuracy": float(accuracy)})

def log_statistics_to_db(engine, data, time):
    logger.info(f"Logging statistics for time: {time}")
    numeric_cols = data.select_dtypes(include=np.number).columns
    for col in numeric_cols:
        if col != 'Class' and col != 'predicted_class':
            mean_val = data[col].mean()
            std_val = data[col].std()
            with engine.begin() as connection:
                connection.execute(text("""
                    INSERT INTO data_statistics (time, feature, mean, std)
                    VALUES (:time, :feature, :mean, :std)
                """),
                    {"time": time, "feature": col, "mean": float(mean_val), "std": float(std_val)})

def simulate_data_stream(data_source, model, scaler, columns_to_scale, num_new_samples=10):
    logger.info(f"Simulating data stream with {num_new_samples} samples")
    new_data_list = []
    for _ in range(num_new_samples):
        new_transaction = pd.DataFrame(
            np.random.normal(size=(1, 28)),
            columns=[f'V{i}' for i in range(1, 29)]
        )
        new_transaction['Amount'] = np.random.normal(size=1)
        new_transaction['Class'] = np.random.choice([0, 1], size=1)

        # Приводим имена к нижнему регистру
        new_transaction.columns = new_transaction.columns.str.lower()

        new_transaction_scaled = scaler.transform(new_transaction.drop('class', axis=1))
        new_transaction['predicted_class'] = model.predict(new_transaction_scaled)

        # Если предсказание не совпадает — дообучаем модель
        if new_transaction['class'].values[0] != new_transaction['predicted_class'].values[0]:
            model.fit(
                scaler.transform(pd.concat([
                    data_source[columns_to_scale],
                    new_transaction[[f'v{i}' for i in range(1, 29)] + ['amount']]
                ])),
                pd.concat([data_source['class'], new_transaction['class']])
            )
        new_data_list.append(new_transaction)

    return pd.concat(new_data_list, ignore_index=True)

def process_data():
    try:
        logger.info("Starting data processing...")

        # Проверяем наличие файла
        data_file = '/app/data/test_creditcard_2023.csv'
        if not os.path.exists(data_file):
            logger.error(f"Data file not found: {data_file}")
            raise FileNotFoundError(f"Data file not found: {data_file}")

        # Загрузка данных
        logger.info(f"Loading data from {data_file}")
        try:
            test_data = pd.read_csv(data_file)
            # Преобразуем имена колонок в нижний регистр
            test_data.columns = test_data.columns.str.lower()
            logger.info(f"Loaded {len(test_data)} records")
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise

        test_data = test_data.dropna()
        logger.info("Data cleaned")

        columns_to_scale = [
            'v1','v2','v3','v4','v5','v6','v7','v8','v9','v10',
            'v11','v12','v13','v14','v15','v16','v17','v18','v19',
            'v20','v21','v22','v23','v24','v25','v26','v27','v28',
            'amount'
        ]

        X = test_data[columns_to_scale]
        y = test_data['class']  # в нижнем регистре уже
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)

        logger.info("Training model...")
        model = RandomForestClassifier(random_state=42, class_weight='balanced')
        model.fit(X_train_scaled, y_train)

        X_test_scaled = scaler.transform(X_test)
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

        logger.info("Initial model evaluation:")
        logger.info("\nClassification Report (initial):")
        logger.info(classification_report(y_test, y_pred))
        logger.info(f"ROC AUC Score (initial): {roc_auc_score(y_test, y_pred_proba)}")

        num_samples = 1000
        start_date = '2025-01-01'
        frequency = '5min'
        time_stamps = pd.date_range(start=start_date, periods=num_samples, freq=frequency)

        logger.info("Generating synthetic data...")
        fraud_proportion = test_data['class'].value_counts(normalize=True)[1]

        normal_mean_V = test_data[test_data['class'] == 0].loc[:, 'v1':'v28'].mean()
        normal_std_V = test_data[test_data['class'] == 0].loc[:, 'v1':'v28'].std()
        normal_mean_amount = test_data[test_data['class'] == 0]['amount'].mean()
        normal_std_amount = test_data[test_data['class'] == 0]['amount'].std()

        fraud_mean_V = test_data[test_data['class'] == 1].loc[:, 'v1':'v28'].mean()
        fraud_std_V = test_data[test_data['class'] == 1].loc[:, 'v1':'v28'].std()
        fraud_mean_amount = test_data[test_data['class'] == 1]['amount'].mean()
        fraud_std_amount = test_data[test_data['class'] == 1]['amount'].std()

        normal_samples = int(num_samples * (1 - fraud_proportion))
        fraud_samples = num_samples - normal_samples

        normal_transactions_V = np.random.normal(normal_mean_V, normal_std_V, (normal_samples, 28))
        normal_amounts = np.random.normal(normal_mean_amount, normal_std_amount, normal_samples)

        fraud_transactions_V = np.random.normal(fraud_mean_V, fraud_std_V, (fraud_samples, 28))
        fraud_amounts = np.random.normal(fraud_mean_amount, fraud_std_amount, fraud_samples)

        synthetic_V = np.vstack([normal_transactions_V, fraud_transactions_V])
        synthetic_amount = np.hstack([normal_amounts, fraud_amounts])
        synthetic_class = np.hstack([np.zeros(normal_samples), np.ones(fraud_samples)])

        synthetic_data = pd.DataFrame(synthetic_V, columns=[f'v{i}' for i in range(1, 29)])
        synthetic_data['amount'] = synthetic_amount
        synthetic_data['class'] = synthetic_class
        synthetic_data.index = time_stamps
        synthetic_data = synthetic_data.sample(frac=1).reset_index()
        synthetic_data.rename(columns={'index': 'time'}, inplace=True)

        logger.info("Processing transactions...")
        for time in synthetic_data['time']:
            logger.info(f"Processing time: {time}")
            current_data = synthetic_data[synthetic_data['time'] == time].copy()
            current_data_scaled = scaler.transform(current_data[columns_to_scale])
            current_data['predicted_class'] = model.predict(current_data_scaled)

            logger.info(f"Saving transaction to database for time {time}")
            try:
                current_data.to_sql('transactions', con=engine, if_exists='append', index=False)
                logger.info("Transaction saved successfully")
            except Exception as e:
                logger.error(f"Error saving transaction: {e}")
                raise

            precision, recall, f1, accuracy = calculate_metrics(
                current_data['class'],
                current_data['predicted_class']
            )
            log_metrics_to_db(engine, precision, recall, f1, accuracy)
            log_statistics_to_db(engine, current_data, time)

            logger.info("Simulating new transactions...")
            # Передаём columns_to_scale в simulate_data_stream:
            new_data = simulate_data_stream(
                synthetic_data,
                model,
                scaler,
                columns_to_scale,   # <-- ВАЖНО: добавили сюда список колонок
                num_new_samples=10
            )
            new_data_scaled = scaler.transform(new_data.drop(['class', 'predicted_class'], axis=1))
            y_pred = model.predict(new_data_scaled)

            logger.info("\nClassification Report (new data):")
            logger.info(classification_report(new_data['class'], y_pred))
            logger.info("----")

            for time_new in new_data.index:
                current_data = new_data.loc[[time_new]].copy()
                current_data_scaled = scaler.transform(
                    current_data.drop(['class', 'predicted_class'], axis=1)
                )
                current_data['predicted_class'] = model.predict(current_data_scaled)
                current_data['time'] = pd.Timestamp.now()

                logger.info("Saving new transaction to database...")
                try:
                    current_data.to_sql('transactions', con=engine, if_exists='append', index=False)
                    logger.info("New transaction saved successfully")
                except Exception as e:
                    logger.error(f"Error saving new transaction: {e}")
                    raise

                precision, recall, f1, accuracy = calculate_metrics(
                    current_data['class'],
                    current_data['predicted_class']
                )
                log_metrics_to_db(engine, precision, recall, f1, accuracy)
                log_statistics_to_db(engine, current_data, pd.Timestamp.now())

            time_module.sleep(10)

    except Exception as e:
        logger.error(f"Error in process_data: {e}")
        raise

@app.route('/')
def home():
    return jsonify({
        "status": "running",
        "time": datetime.now().isoformat()
    })

@app.route('/metrics')
def get_metrics():
    try:
        with engine.connect() as connection:
            result = connection.execute(text("""
                SELECT precision, recall, f1, accuracy
                FROM model_metrics
                ORDER BY time DESC
                LIMIT 1
            """))
            metrics = result.fetchone()

            if metrics is None:
                return jsonify({
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1': 0.0,
                    'accuracy': 0.0,
                    'message': 'No metrics available yet'
                })

            return jsonify({
                'precision': float(metrics[0]),
                'recall': float(metrics[1]),
                'f1': float(metrics[2]),
                'accuracy': float(metrics[3])
            })
    except Exception as e:
        logger.error(f"Error in /metrics: {e}")
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/statistics')
def get_statistics():
    try:
        with engine.connect() as connection:
            result = connection.execute(text("""
                SELECT feature, mean, std
                FROM data_statistics
                ORDER BY time DESC
                LIMIT 28
            """))
            stats = result.fetchall()

            return jsonify({
                'statistics': [
                    {
                        'feature': stat[0],
                        'mean': float(stat[1]),
                        'std': float(stat[2])
                    }
                    for stat in stats
                ]
            })
    except Exception as e:
        logger.error(f"Error in /statistics: {e}")
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/transactions')
def get_transactions():
    try:
        with engine.connect() as connection:
            result = connection.execute(text("""
                SELECT time, amount, class, predicted_class
                FROM transactions
                ORDER BY time DESC
                LIMIT 100
            """))
            transactions = result.fetchall()

            return jsonify({
                'transactions': [
                    {
                        'time': tr[0].isoformat(),
                        'amount': float(tr[1]),
                        'actual_class': int(tr[2]),
                        'predicted_class': int(tr[3])
                    }
                    for tr in transactions
                ]
            })
    except Exception as e:
        logger.error(f"Error in /transactions: {e}")
        return jsonify({
            'error': str(e)
        }), 500

if __name__ == "__main__":
    logger.info("Starting application...")

    def run_data_processing():
        try:
            logger.info("Starting data processing thread...")
            wait_for_db()
            create_initial_tables(engine)
            process_data()
        except Exception as e:
            logger.error(f"Error in data processing thread: {e}")
            raise

    processing_thread = Thread(target=run_data_processing)
    processing_thread.daemon = True
    processing_thread.start()
    logger.info("Data processing thread started")
    app.run(host='0.0.0.0', port=8000, debug=False)